{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Recognition from Audio Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data set can be downloaded from : https://research.google.com/audioset/dataset/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets install Libraries which might not be there. Do remove the hashtags to uncomment the code so that the installation happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install librosa\n",
    "# !pip install python_speech_features\n",
    "# !pip install sounddevice\n",
    "# !pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Feature extraction\n",
    "import scipy\n",
    "import librosa\n",
    "import python_speech_features as mfcc\n",
    "import os\n",
    "from scipy.io.wavfile import read\n",
    "\n",
    "# Model training\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Live recording\n",
    "import sounddevice as sd\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MFCC â€” Mel-Frequency Cepstral Coefficients\n",
    "\n",
    "The first step in any automatic speech recognition system is to extract features i.e. identify the components of the audio signal that are good for identifying the linguistic content and discarding all the other stuff which carries information like background noise, emotion etc.\n",
    "\n",
    "The main point to understand about speech is that the sounds generated by a human are filtered by the shape of the vocal tract including tongue, teeth etc. This shape determines what sound comes out. If we can determine the shape accurately, this should give us an accurate representation of the phoneme being produced. The shape of the vocal tract manifests itself in the envelope of the short time power spectrum, and the job of MFCCs is to accurately represent this envelope. By printing the shape of mfccs you get how many mfccs are calculated on how many frames.\n",
    "\n",
    "Extracting the MFCC of a audio file is really easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MFCC(sr,audio):\n",
    "    \n",
    "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = False)\n",
    "    features = preprocessing.scale(features)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in a folder called AudioSet, in which there are two sub-folders: male_clips and female_clips. We can extract the features of the training set simply by running the function above on all files in the training folder. The problem is however that for the moment, both the train and the test set are in the folder. \n",
    "\n",
    "We must, therefore, split these files in two, and run get_MFCC iteratively,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(source):\n",
    "    \n",
    "    # Split files\n",
    "    files = [os.path.join(source,f) for f in os.listdir(source) if f.endswith('.wav')]\n",
    "    len_train = int(len(files)*0.8) #we will take only 80% of data for training and remaining 10% for testing and 10% for validation\n",
    "    len_valortest = int(len(files)*0.1) \n",
    "    train_files = files[:len_train]\n",
    "    testval_files = files[len_train:]\n",
    "    test_files = testval_files[:len_valortest]\n",
    "    val_files = testval_files[len_valortest:]\n",
    "    \n",
    "    # Train features\n",
    "    features_train = []\n",
    "    for f in train_files:\n",
    "        sr, audio = read(f)\n",
    "        vector = get_MFCC(sr,audio) #using the function we defined above to get features.\n",
    "        if len(features_train) == 0:\n",
    "            features_train = vector\n",
    "        else:\n",
    "            features_train = np.vstack((features_train, vector)) #The vstack() function is used to stack arrays in sequence vertically (row wise)\n",
    "            \n",
    "    # Test features  \n",
    "    features_test = []\n",
    "    for f in test_files:\n",
    "        sr, audio = read(f)\n",
    "        vector = get_MFCC(sr,audio)\n",
    "        if len(features_test) == 0:\n",
    "            features_test = vector\n",
    "        else:\n",
    "            features_test = np.vstack((features_test, vector))\n",
    "            \n",
    "    # Val features  \n",
    "    features_val = []\n",
    "    for f in val_files:\n",
    "        sr, audio = read(f)\n",
    "        vector = get_MFCC(sr,audio)\n",
    "        if len(features_val) == 0:\n",
    "            features_val = vector\n",
    "        else:\n",
    "            features_val = np.vstack((features_val, vector))\n",
    "            \n",
    "    return features_train, features_test, features_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting male audio data from folder,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"male_clips\"\n",
    "features_train_male, features_test_male, features_val_male = get_features(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for females,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"female_clips\"\n",
    "features_train_female, features_test_female, features_val_female =  get_features(source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the training and the validantion data numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "for i in features_train_male:\n",
    "    X.append(i)\n",
    "    y.append(1)\n",
    "for i in features_train_female:\n",
    "    X.append(i)\n",
    "    y.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = []\n",
    "y_val = []\n",
    "for i in features_val_male:\n",
    "    X_val.append(i)\n",
    "    y_val.append(1)\n",
    "for i in features_val_female:\n",
    "    X_val.append(i)\n",
    "    y_val.append(0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "y_test = []\n",
    "for i in features_test_male:\n",
    "    X_test.append(i)\n",
    "    y_test.append(1)\n",
    "for i in features_test_female:\n",
    "    X_test.append(i)\n",
    "    y_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model\n",
    "We are going to use a deep feed-forward neural network with 6 hidden layers, it isn't the perfect architecture, but it does the job so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"6 hidden dense layers from 512 units to 64, not the best model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(None,13)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(128, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    # one output neuron with sigmoid activation function, 0 means female, 1 means male\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    # using binary crossentropy as it's male/female classification (binary)\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=[\"accuracy\"], optimizer=\"adam\")\n",
    "    # print summary of the model\n",
    "    model.summary()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using a 30% dropout rate after each fully connected layer, this type of regularization will hopefully prevent overfitting on the training dataset.\n",
    "\n",
    "An important thing to note here is we're using a single output unit (neuron) with a sigmoid activation function in the output layer, the model will output the scalar 1 (or close to it) when the audio's speaker is a male, and female when it's closer to 0.\n",
    "\n",
    "Also, we're using binary cross entropy as the loss function, as it is a special case of categorical cross entropy when we only have 2 classes to predict. Let's use this function to build our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, None, 512)         7168      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, None, 256)         131328    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, None, 128)         16512     \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, None, 64)          8256      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 458,881\n",
      "Trainable params: 458,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two callbacks that will get executed after the end of each epoch:\n",
    "\n",
    "The first is the tensorboard, we gonna use it to see how the model goes during the training in terms of loss and accuracy.\n",
    "The second callback is early stopping, this will stop the training when the model stops improving, a patience of 5 is specified, which means it will stop training after 5 epochs of not improving, setting restore_best_weights to True will restore the optimal weights that was recorded during the training and assign it to the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 13) for input Tensor(\"dense_14_input:0\", shape=(None, None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 13) for input Tensor(\"dense_14_input:0\", shape=(None, None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "   2/1756 [..............................] - ETA: 13:38 - loss: 0.6912 - accuracy: 0.5140WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0300s vs `on_train_batch_end` time: 0.8893s). Check your callbacks.\n",
      "1754/1756 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.6568WARNING:tensorflow:Model was constructed with shape (None, None, 13) for input Tensor(\"dense_14_input:0\", shape=(None, None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "1756/1756 [==============================] - 42s 24ms/step - loss: 0.6157 - accuracy: 0.6568 - val_loss: 0.6036 - val_accuracy: 0.6661\n",
      "Epoch 2/50\n",
      "1756/1756 [==============================] - 42s 24ms/step - loss: 0.5997 - accuracy: 0.6728 - val_loss: 0.5988 - val_accuracy: 0.6686\n",
      "Epoch 3/50\n",
      "1756/1756 [==============================] - 42s 24ms/step - loss: 0.5946 - accuracy: 0.6777 - val_loss: 0.5989 - val_accuracy: 0.6713\n",
      "Epoch 4/50\n",
      "1756/1756 [==============================] - 41s 23ms/step - loss: 0.5914 - accuracy: 0.6803 - val_loss: 0.5965 - val_accuracy: 0.6728\n",
      "Epoch 5/50\n",
      "1756/1756 [==============================] - 44s 25ms/step - loss: 0.5884 - accuracy: 0.6826 - val_loss: 0.5946 - val_accuracy: 0.6727\n",
      "Epoch 6/50\n",
      "1756/1756 [==============================] - 39s 22ms/step - loss: 0.5864 - accuracy: 0.6840 - val_loss: 0.5987 - val_accuracy: 0.6713\n",
      "Epoch 7/50\n",
      "1756/1756 [==============================] - 45s 26ms/step - loss: 0.5850 - accuracy: 0.6846 - val_loss: 0.5977 - val_accuracy: 0.6701\n",
      "Epoch 8/50\n",
      "1756/1756 [==============================] - 40s 23ms/step - loss: 0.5833 - accuracy: 0.6865 - val_loss: 0.5988 - val_accuracy: 0.6708\n",
      "Epoch 9/50\n",
      "1756/1756 [==============================] - 41s 24ms/step - loss: 0.5819 - accuracy: 0.6874 - val_loss: 0.6030 - val_accuracy: 0.6719\n",
      "Epoch 10/50\n",
      "1756/1756 [==============================] - 42s 24ms/step - loss: 0.5808 - accuracy: 0.6883 - val_loss: 0.5980 - val_accuracy: 0.6729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e406d43f70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use tensorboard to view metrics\n",
    "tensorboard = TensorBoard(log_dir=\"logs\")\n",
    "# define early stopping to stop training after 5 epochs of not improving\n",
    "early_stopping = EarlyStopping(mode=\"min\", patience=5, restore_best_weights=True)\n",
    "\n",
    "batch_size = 500\n",
    "epochs = 50\n",
    "# train the model using the training set and validating using validation set\n",
    "model.fit(X, y, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val),\n",
    "          callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model now is trained and the weights are optimal, let's test it using our testing set we created earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file\n",
    "model.save(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5982\n",
      "Accuracy: 66.70%\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model using the testing set\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Loss: {loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model with your own Voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"results/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, None, 512)         7168      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, None, 512)         262656    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, None, 256)         131328    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, None, 256)         0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, None, 128)         32896     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, None, 128)         16512     \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, None, 64)          8256      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, None, 64)          0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, None, 1)           65        \n",
      "=================================================================\n",
      "Total params: 458,881\n",
      "Trainable params: 458,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_predict(sr=16000, channels=1, duration=3, filename='pred_record.wav'):\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(\"Recording Started...\")\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    recording = sd.rec(int(duration * sr), samplerate=sr, channels=channels).reshape(-1)\n",
    "    sd.wait()\n",
    "    print(\"Recording ended...\")\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    features = get_MFCC(sr,recording)\n",
    "    m = model.predict(features)\n",
    "    m = np.mean(m)\n",
    "    f = (1-m)*100\n",
    "    m = m * 100\n",
    "    if m>f:\n",
    "        print(\"THE SPEAKER IS MALE\")\n",
    "    else:\n",
    "        print(\"THE SPEAKER IS FEMALE\")\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "Recording Started...\n",
      "-----------------------------------------------------------------------------------\n",
      "Recording ended...\n",
      "-----------------------------------------------------------------------------------\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 13) for input Tensor(\"dense_14_input:0\", shape=(None, None, 13), dtype=float32), but it was called on an input with incompatible shape (None, 13).\n",
      "THE SPEAKER IS MALE\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "record_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
